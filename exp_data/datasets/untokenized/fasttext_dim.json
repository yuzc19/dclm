{
    "uuid": "4b7153b7-b133-4e5f-ad7e-a87924cf435b",
    "name": "fasttext_dim",
    "creation_date": "2024_09_05-22_46_58",
    "dataset_url": "../tmp/dclm/output/refinedweb_01_0/fasttext/fasttext_filter/2/10000-data_influence_model/processed_data/",
    "manifest_url": null,
    "sources": [
        {
            "uuid": "f12dc026-cc4a-4203-ba9f-9ba08c5945f9",
            "name": "baseline_01_0"
        }
    ],
    "tokenized": false,
    "tokenizer": null,
    "num_tokens": null,
    "size": 0,
    "dcnlp_commit_hash": "294684af36b3c8fdbefe04d9fabafc05fd9ac55a",
    "dcnlp_diff": "diff --git a/baselines/mappers/enrichers/__init__.py b/baselines/mappers/enrichers/__init__.py\nindex aab426b..c457e9e 100644\n--- a/baselines/mappers/enrichers/__init__.py\n+++ b/baselines/mappers/enrichers/__init__.py\n@@ -2,3 +2,4 @@ from .enrichers import line_counter_enricher\n from .language_id_enrichers import detect_lang_paragraph_enricher, detect_lang_whole_page_enricher\n from .quality_prediction_enrichers_calc_fasttext import classify_fasttext_hq_prob_enricher\n from .quality_prediction_enrichers_kenlm_model import ken_lm_perplexity_enricher\n+from .dim_enrichers import assign_dim_score_enricher\n\\ No newline at end of file\ndiff --git a/baselines/mappers/enrichers/quality_prediction_enrichers_calc_fasttext.py b/baselines/mappers/enrichers/quality_prediction_enrichers_calc_fasttext.py\nindex 5231bce..26a77d5 100644\n--- a/baselines/mappers/enrichers/quality_prediction_enrichers_calc_fasttext.py\n+++ b/baselines/mappers/enrichers/quality_prediction_enrichers_calc_fasttext.py\n@@ -1,20 +1,24 @@\n-'''\n+\"\"\"\n This script classifies a given text as either 'CC' or 'Wikipedia' using a FastText model from the RedPajama project.\n More details about the model can be found in this GitHub issue: https://github.com/togethercomputer/RedPajama-Data/issues/24\n Model download is done via `setup.py` script.\n The download link is: https://drive.google.com/file/d/1DnsfpWWE0jFPCoYe6clwqb3Ub5Ac92s1/view?usp=share_link\n-'''\n+\"\"\"\n+\n import os\n-from typing import Dict, List, Callable\n+from typing import Callable, Dict, List\n \n import fasttext\n \n from core.constants import CONTENT\n from core.factory_utils import factory_function\n \n-PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\n+PROJECT_ROOT = os.path.abspath(\n+    os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\")\n+)\n MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\n-RPJ_MODEL_FILENAME = 'model.bin'\n+RPJ_MODEL_FILENAME = \"model.bin\"\n+\n \n def load_fasttext_model(model_filename):\n     if os.path.exists(MODEL_SUBDIRECTORY):\n@@ -30,8 +34,11 @@ def load_fasttext_model(model_filename):\n \n     return fasttext.load_model(model_path)\n \n-def classify_fasttext_hq_prob(model: fasttext.FastText._FastText, content: str) -> dict:\n-    '''\n+\n+def classify_fasttext_hq_prob(\n+    model: fasttext.FastText._FastText, content: str, lq_label: str\n+) -> dict:\n+    \"\"\"\n     This function classifies a given text as either 'CC' or 'Wikipedia' and returns the label along with its probability.\n \n     Parameters:\n@@ -40,7 +47,7 @@ def classify_fasttext_hq_prob(model: fasttext.FastText._FastText, content: str)\n \n     Returns:\n     dict: A value for 'hq_prob' - the probability to be a high-quality page.\n-    '''\n+    \"\"\"\n \n     # Initialize an empty dictionary for the output\n     output = {}\n@@ -57,7 +64,7 @@ def classify_fasttext_hq_prob(model: fasttext.FastText._FastText, content: str)\n     hq_prob = pred_prob[0]\n \n     # If the predicted label is 'CC', adjust the probability of it being 'Wikipedia'\n-    if pred_label == \"__label__cc\":\n+    if pred_label == lq_label:\n         hq_prob = 1 - hq_prob\n \n     # Return the output\n@@ -65,9 +72,13 @@ def classify_fasttext_hq_prob(model: fasttext.FastText._FastText, content: str)\n \n \n @factory_function\n-def classify_fasttext_hq_prob_enricher(model_filename=RPJ_MODEL_FILENAME, key: str = \"fasttext_hq_prob\", overwrite: bool = False) -> Callable[\n-    [Dict], List[Dict]]:\n-    '''\n+def classify_fasttext_hq_prob_enricher(\n+    model_filename=RPJ_MODEL_FILENAME,\n+    key: str = \"fasttext_hq_prob\",\n+    overwrite: bool = False,\n+    lq_label: str = \"__label__cc\",\n+) -> Callable[[Dict], List[Dict]]:\n+    \"\"\"\n     Enriches the given page with the text type (CC or Wikipedia).\n \n     Parameters:\n@@ -78,12 +89,16 @@ def classify_fasttext_hq_prob_enricher(model_filename=RPJ_MODEL_FILENAME, key: s\n \n     Returns:\n         A function that enriches the given page with the text type (HQ or CC).\n-    '''\n+    \"\"\"\n     model = load_fasttext_model(model_filename)\n \n     def enrich(page: Dict) -> List[Dict]:\n         assert overwrite or key not in page, f\"cannot overwrite an existing key {key}\"\n-        page[key] = classify_fasttext_hq_prob(model, page[CONTENT])\n+        page[key] = classify_fasttext_hq_prob(\n+            model,\n+            page[CONTENT],\n+            lq_label,\n+        )\n         return [page]\n \n     return enrich\ndiff --git a/eval/eval_openlm_ckpt.py b/eval/eval_openlm_ckpt.py\nindex b4a5cba..edee8e6 100644\n--- a/eval/eval_openlm_ckpt.py\n+++ b/eval/eval_openlm_ckpt.py\n@@ -260,6 +260,7 @@ def dump_or_update_output(args, local_rank, eval_metrics=None, helm_eval_metrics\n     print(\"Eval output: \")\n     print(json.dumps(output, indent=4, sort_keys=True))\n     if local_rank == 0:\n+        Path(args.output_file).parent.mkdir(parents=True, exist_ok=True)\n         with open(args.output_file, \"w\") as f:\n             json.dump(output, f, indent=4)\n \n@@ -519,11 +520,11 @@ def main():\n     args.remote_sync = args.output_file\n     directory = os.path.dirname(args.output_file)\n     if directory != \"\" and not os.path.exists(directory):\n-        os.makedirs(directory)\n+        os.makedirs(directory, exist_ok=True)\n \n     CWD = os.getcwd()\n     if args.use_temp_working_dir:\n-        temp_dir = os.path.join(CWD, \"eval_openlm_ckpt_temp_dirs\", f\"{uuid.uuid4()}\")\n+        temp_dir = os.path.join(CWD, \"eval_openlm_ckpt_temp_dirs\")\n         os.makedirs(temp_dir, exist_ok=True)  # in case rank > 0\n         os.chdir(temp_dir)\n         print(f\"Using temporary working directory: {temp_dir}\")\n@@ -617,6 +618,7 @@ def main():\n     icl_results = evaluate(eval_model, tokenizer, eval_cfg)\n     eval_metrics[\"icl\"] = icl_results\n \n+    os.chdir(\"..\")\n     dump_or_update_output(args, local_rank, eval_metrics=eval_metrics)\n \n \ndiff --git a/exp_data/datasets/raw_sources/test.json b/exp_data/datasets/raw_sources/test.json\ndeleted file mode 100644\nindex a86b969..0000000\n--- a/exp_data/datasets/raw_sources/test.json\n+++ /dev/null\n@@ -1,13 +0,0 @@\n-{\n-    \"uuid\": \"f12dc026-cc4a-4203-ba9f-9ba08c5945f9\",\n-    \"name\": \"baseline_01_0\",\n-    \"dataset_url\": \"s3://commoncrawl/contrib/datacomp/DCLM-refinedweb/global-shard_01_of_10/local-shard_0_of_10\",\n-    \"manifest_url\": null,\n-    \"sources\": [],\n-    \"tokenized\": false,\n-    \"tokenizer\": null,\n-    \"num_tokens\": null,\n-    \"dcnlp_commit_hash\": null,\n-    \"dcnlp_diff\": null,\n-    \"data_key\": \"jsonl.zstd\"\n-}\ndiff --git a/note.md b/note.md\nindex 8be5204..446a965 100644\n--- a/note.md\n+++ b/note.md\n@@ -12,6 +12,30 @@ Run setup.py to download necessary files:\n python setup.py install\n ```\n \n+Go to: http://aws.amazon.com/\n+Sign Up & create a new account (they'll give you the option for 1 year trial or similar)\n+Go to your AWS account overview\n+Account menu in the upper-right (has your name on it)\n+sub-menu: Security Credentials\n+\n+```bash\n+curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+unzip awscliv2.zip\n+sudo ./aws/install\n+\n+aws configure\n+\n+# DCLM-refinedweb\t(one local shard = 405.6 GiB), so the total is around 41.6 TiB, while the entire is 279.6 TiB\n+# DCLM-baseline\t6.6 TiB\n+# 1: 240T tokens, 279.6 TiB (5,047,684), 340TB, 370TB after gzip compression\n+# 2: 16.98T tokens (357,163)\n+# 3: 100B tokens (needed for 400M-1x at 2)\n+# 4: 8.2B tokens (final 400M-1x)\n+# The files in each shard were shuffled before the dataset was split into shards. The documents within each file were not further shuffled - this global shuffle occurs later in our pipeline, after filtering and tokenization of the dataset. If global shuffle before tokenization across all the documents is required by your processing scheme, make sure to take this into account.\n+# The documents were initially written into files as the were being read and processed from the CommonCrawl WARC files, so there was indeed no shuffling at this initial stage. After the files were created, we shuffled them (at the file level) and then split them into shards. However, because shuffling never happened at the document level at this stage, picking e.g. 300M documents at random from the entire dataset is not exactly the same as picking one shard.\n+with-proxy aws s3 ls --summarize --human-readable --recursive s3://commoncrawl/contrib/datacomp/DCLM-refinedweb/global-shard_01_of_10/local-shard_0_of_10/\n+```\n+\n ### Ray\n \n To launch a local ray cluster, use the following command:\n@@ -70,3 +94,21 @@ python ray_processing/process.py \\\n ### Step3: Model-based ltering\n \n Similar to step1, but use `baselines/baselines_configs/fasttext_filter.yaml` instead.\n+\n+\n+- \"output/cc_wet_2019_april_baselines/refinedweb/refinedweb/processed_data\"\n+\n+\n+```\n+import os\n+\n+import datasets\n+\n+# /data/users/zichunyu/data/hf_cache/mlfoundations___json/mlfoundations--dclm-pool-400m-1x-36757e8d7b7ffd23/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n+datasets.load_dataset(\n+    \"mlfoundations/dclm-pool-400m-1x\",\n+    cache_dir=\"data\",\n+    num_proc=os.cpu_count() - 1,\n+)\n+\n+```\ndiff --git a/ray_processing/process.py b/ray_processing/process.py\nindex 58cd867..c77deb7 100644\n--- a/ray_processing/process.py\n+++ b/ray_processing/process.py\n@@ -142,7 +142,11 @@ if __name__ == \"__main__\":\n     if args.ray_use_working_dir:\n         ray.init(address=args.ray_address, runtime_env={\"working_dir\": \"./\", \"excludes\": [\"tests/\"]})\n     else:\n-        ray.init(address=args.ray_address)\n+        ray.init(\n+            runtime_env={\"env_vars\": {k: v for k, v in os.environ.items() if k.startswith(\"AWS\")}},\n+            _temp_dir=\"/data/users/zichunyu/tmp/ray\",\n+            dashboard_host=\"127.0.0.1\",\n+        )\n \n     config_path = args.config_path\n     output_dir = args.output_dir\n@@ -207,6 +211,8 @@ if __name__ == \"__main__\":\n \n         # Retrieve the list of files before processing a chunk (in case of deletions)\n         shard_files = list_shard_files(working_dir, args.num_shards, args.shard_list_file)\n+        # shard_files = shard_files[:1]\n+        # shard_files = [\"CC_shard_00000000_processed.jsonl.zst\"]\n         shard_extension = os.path.splitext(shard_files[0])[-1][1:]\n         print(f\"Starting chunk {i} with name {step_name}, # of input jsonls = {len(shard_files)}\")\n \ndiff --git a/ray_processing/tokenize_shuffle.py b/ray_processing/tokenize_shuffle.py\nindex 6a029d7..bbe9ba3 100644\n--- a/ray_processing/tokenize_shuffle.py\n+++ b/ray_processing/tokenize_shuffle.py\n@@ -102,9 +102,10 @@ def main(args, dcnlp_arg_names):\n     dataset_json = generate_tokenized_dataset_json(args, source_refs)\n     with open(json_path, \"w\") as ref_file:\n         json.dump(dataset_json, ref_file, indent=4)\n-    out_json_path = f\"{args.output}/{pathlib.Path(args.output).name}.json\"\n-    print(f\"moving dataset json to {out_json_path}\")\n-    os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n+    # out_json_path = f\"{args.output}/{pathlib.Path(args.output).name}.json\"\n+    # os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n+    out_json_path = f\"{args.output.rstrip('/')}/{pathlib.Path(args.output).name}.json\"\n+    os.system(f\"cp {json_path} {out_json_path}\")\n \n \n if __name__ == \"__main__\":\ndiff --git a/ray_processing/utils.py b/ray_processing/utils.py\nindex f835848..a068a37 100644\n--- a/ray_processing/utils.py\n+++ b/ray_processing/utils.py\n@@ -30,9 +30,12 @@ def get_source_ref(source_ref_path):\n     with open(source_ref_path, \"r\") as file:\n         return json.load(file)\n \n-\n def count_tokens(manifest_url, seqlen=2049):\n-    with S3Path(manifest_url).open(\"r\") as f:\n+    with (\n+        S3Path(manifest_url).open(\"r\")\n+        if manifest_url.startswith(\"s3://\")\n+        else open(manifest_url, \"r\")\n+    ) as f:\n         manifest = [json.loads(line) for line in f]\n     num_tokens = sum(int(line[\"num_sequences\"]) for line in manifest) * seqlen\n     return num_tokens\n@@ -46,13 +49,20 @@ def get_s3_dir_size(dataset_path):\n     return total_size\n \n \n+def get_dir_size(dataset_path):\n+    total_size = 0\n+    for filename in os.listdir(dataset_path):\n+        file_path = os.path.join(dataset_path, filename)\n+        if os.path.isfile(file_path):\n+            total_size += os.path.getsize(file_path)\n+    return total_size\n+\n def get_git_info():\n     repo = git.Repo(search_parent_directories=True)\n     dcnlp_commit_hash = repo.head.object.hexsha\n     dcnlp_diff = repo.git.diff(repo.head.commit.tree)\n     return dcnlp_commit_hash, dcnlp_diff\n \n-\n def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_key=\".json.zstd\"):\n     sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n     dcnlp_commit_hash, dcnlp_diff = get_git_info()\ndiff --git a/training/configs/1b_1x_fast.json b/training/configs/1b_1x_fast.json\nindex 9890c8c..9441dbf 100644\n--- a/training/configs/1b_1x_fast.json\n+++ b/training/configs/1b_1x_fast.json\n@@ -8,7 +8,7 @@\n     \"wd\": 0.033,\n     \"cd\": 3e-5,\n     \"global_bs\": 256,\n-    \"acc\": 1,\n+    \"acc\": 4,\n     \"qk_norm\": true,\n     \"z_loss\": 1e-4,\n     \"grad_checkpointing\": false,\ndiff --git a/training/params.py b/training/params.py\nindex 9b92353..cf36f5f 100644\n--- a/training/params.py\n+++ b/training/params.py\n@@ -285,7 +285,8 @@ def get_open_lm_args(args, hparams, dr):\n         f\"{hparams.acc}\",\n         \"--model-norm\",\n         hparams.norm,\n-        \"--delete-previous-checkpoint\",\n+        # delete previous ones?\n+        # \"--delete-previous-checkpoint\",\n         \"--lr-cooldown-end\",\n         f\"{hparams.cd}\",\n         \"--logs\",\n@@ -315,7 +316,6 @@ def get_open_lm_args(args, hparams, dr):\n     if args.re_evaluate is None:\n         # case where we are training\n         name = hparams.get_friendly_name(dr, args.name_suffix)\n-\n         open_lm_args.extend(\n             [\n                 \"--train-num-samples\",\n@@ -347,20 +347,20 @@ def get_open_lm_args(args, hparams, dr):\n \n     if args.do_eval:\n         openlm_val_data = download_val_data(\"open_lm_val\", skip_download=local_rank != 0)\n-        c4_val_data = download_val_data(\"c4_val\", skip_download=local_rank != 0)\n-        paloma_val_data = download_val_data(\"paloma_val\", skip_download=local_rank != 0)\n+        # c4_val_data = download_val_data(\"c4_val\", skip_download=local_rank != 0)\n+        # paloma_val_data = download_val_data(\"paloma_val\", skip_download=local_rank != 0)\n \n     if not args.do_eval:\n         pass\n     elif args.downstream_eval:\n-        tasks = load_ppl_yaml()\n+        tasks = load_ppl_yaml(\"light\")\n         downstream_datas = [download_val_data(task_name, skip_download=local_rank != 0) for task_name in tasks]\n \n         open_lm_args.extend(\n             [\n                 \"--val-data\",\n                 openlm_val_data,\n-                c4_val_data,\n+                # c4_val_data,\n                 # paloma_val_data,\n                 *downstream_datas,\n                 \"--val-frequency\",",
    "data_key": "zstd"
}
