{
    "uuid": "c221273f-e343-4328-94a5-67b636292c0f",
    "name": "fasttext",
    "creation_date": "2024_08_27-23_10_06",
    "dataset_url": "output/baseline_01_0/fasttext/fasttext_filter/processed_data/",
    "manifest_url": null,
    "sources": [
        {
            "uuid": "f12dc026-cc4a-4203-ba9f-9ba08c5945f9",
            "name": "baseline_01_0"
        }
    ],
    "tokenized": false,
    "tokenizer": null,
    "num_tokens": null,
    "size": 0,
    "dcnlp_commit_hash": "6eb3a99f5d476e4de366cc5b3dca7c924094caea",
    "dcnlp_diff": "diff --git a/baselines/core/file_utils.py b/baselines/core/file_utils.py\nindex e827a0b..fad61a0 100644\n--- a/baselines/core/file_utils.py\n+++ b/baselines/core/file_utils.py\n@@ -72,6 +72,7 @@ def write_jsonl(data, file_path: str, mode: str = \"w\"):\n     if is_s3(file_path):\n         path = S3Path(file_path)\n     else:\n+        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n         path = LocalPath(file_path)\n \n     if is_compressed(file_path):\ndiff --git a/eval/eval_openlm_ckpt.py b/eval/eval_openlm_ckpt.py\nindex b4a5cba..91272dd 100644\n--- a/eval/eval_openlm_ckpt.py\n+++ b/eval/eval_openlm_ckpt.py\n@@ -523,7 +523,7 @@ def main():\n \n     CWD = os.getcwd()\n     if args.use_temp_working_dir:\n-        temp_dir = os.path.join(CWD, \"eval_openlm_ckpt_temp_dirs\", f\"{uuid.uuid4()}\")\n+        temp_dir = os.path.join(CWD, \"eval_openlm_ckpt_temp_dirs\")\n         os.makedirs(temp_dir, exist_ok=True)  # in case rank > 0\n         os.chdir(temp_dir)\n         print(f\"Using temporary working directory: {temp_dir}\")\ndiff --git a/note.md b/note.md\nindex 8be5204..446a965 100644\n--- a/note.md\n+++ b/note.md\n@@ -12,6 +12,30 @@ Run setup.py to download necessary files:\n python setup.py install\n ```\n \n+Go to: http://aws.amazon.com/\n+Sign Up & create a new account (they'll give you the option for 1 year trial or similar)\n+Go to your AWS account overview\n+Account menu in the upper-right (has your name on it)\n+sub-menu: Security Credentials\n+\n+```bash\n+curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+unzip awscliv2.zip\n+sudo ./aws/install\n+\n+aws configure\n+\n+# DCLM-refinedweb\t(one local shard = 405.6 GiB), so the total is around 41.6 TiB, while the entire is 279.6 TiB\n+# DCLM-baseline\t6.6 TiB\n+# 1: 240T tokens, 279.6 TiB (5,047,684), 340TB, 370TB after gzip compression\n+# 2: 16.98T tokens (357,163)\n+# 3: 100B tokens (needed for 400M-1x at 2)\n+# 4: 8.2B tokens (final 400M-1x)\n+# The files in each shard were shuffled before the dataset was split into shards. The documents within each file were not further shuffled - this global shuffle occurs later in our pipeline, after filtering and tokenization of the dataset. If global shuffle before tokenization across all the documents is required by your processing scheme, make sure to take this into account.\n+# The documents were initially written into files as the were being read and processed from the CommonCrawl WARC files, so there was indeed no shuffling at this initial stage. After the files were created, we shuffled them (at the file level) and then split them into shards. However, because shuffling never happened at the document level at this stage, picking e.g. 300M documents at random from the entire dataset is not exactly the same as picking one shard.\n+with-proxy aws s3 ls --summarize --human-readable --recursive s3://commoncrawl/contrib/datacomp/DCLM-refinedweb/global-shard_01_of_10/local-shard_0_of_10/\n+```\n+\n ### Ray\n \n To launch a local ray cluster, use the following command:\n@@ -70,3 +94,21 @@ python ray_processing/process.py \\\n ### Step3: Model-based ltering\n \n Similar to step1, but use `baselines/baselines_configs/fasttext_filter.yaml` instead.\n+\n+\n+- \"output/cc_wet_2019_april_baselines/refinedweb/refinedweb/processed_data\"\n+\n+\n+```\n+import os\n+\n+import datasets\n+\n+# /data/users/zichunyu/data/hf_cache/mlfoundations___json/mlfoundations--dclm-pool-400m-1x-36757e8d7b7ffd23/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n+datasets.load_dataset(\n+    \"mlfoundations/dclm-pool-400m-1x\",\n+    cache_dir=\"data\",\n+    num_proc=os.cpu_count() - 1,\n+)\n+\n+```\ndiff --git a/ray_processing/process.py b/ray_processing/process.py\nindex 58cd867..207f487 100644\n--- a/ray_processing/process.py\n+++ b/ray_processing/process.py\n@@ -142,7 +142,11 @@ if __name__ == \"__main__\":\n     if args.ray_use_working_dir:\n         ray.init(address=args.ray_address, runtime_env={\"working_dir\": \"./\", \"excludes\": [\"tests/\"]})\n     else:\n-        ray.init(address=args.ray_address)\n+        ray.init(\n+            runtime_env={\"env_vars\": {k: v for k, v in os.environ.items() if k.startswith(\"AWS\")}},\n+            _temp_dir=\"/data/users/zichunyu/tmp/ray\",\n+            dashboard_host=\"127.0.0.1\",\n+        )\n \n     config_path = args.config_path\n     output_dir = args.output_dir\n@@ -207,6 +211,8 @@ if __name__ == \"__main__\":\n \n         # Retrieve the list of files before processing a chunk (in case of deletions)\n         shard_files = list_shard_files(working_dir, args.num_shards, args.shard_list_file)\n+        # shard_files = [\"CC_shard_00000000_processed.jsonl.zst\"]\n+        # shard_files = [\"shard_00001065_processed.jsonl.zstd\"]\n         shard_extension = os.path.splitext(shard_files[0])[-1][1:]\n         print(f\"Starting chunk {i} with name {step_name}, # of input jsonls = {len(shard_files)}\")\n \ndiff --git a/ray_processing/tokenize_shuffle.py b/ray_processing/tokenize_shuffle.py\nindex 6a029d7..bbe9ba3 100644\n--- a/ray_processing/tokenize_shuffle.py\n+++ b/ray_processing/tokenize_shuffle.py\n@@ -102,9 +102,10 @@ def main(args, dcnlp_arg_names):\n     dataset_json = generate_tokenized_dataset_json(args, source_refs)\n     with open(json_path, \"w\") as ref_file:\n         json.dump(dataset_json, ref_file, indent=4)\n-    out_json_path = f\"{args.output}/{pathlib.Path(args.output).name}.json\"\n-    print(f\"moving dataset json to {out_json_path}\")\n-    os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n+    # out_json_path = f\"{args.output}/{pathlib.Path(args.output).name}.json\"\n+    # os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n+    out_json_path = f\"{args.output.rstrip('/')}/{pathlib.Path(args.output).name}.json\"\n+    os.system(f\"cp {json_path} {out_json_path}\")\n \n \n if __name__ == \"__main__\":\ndiff --git a/ray_processing/utils.py b/ray_processing/utils.py\nindex f835848..a068a37 100644\n--- a/ray_processing/utils.py\n+++ b/ray_processing/utils.py\n@@ -30,9 +30,12 @@ def get_source_ref(source_ref_path):\n     with open(source_ref_path, \"r\") as file:\n         return json.load(file)\n \n-\n def count_tokens(manifest_url, seqlen=2049):\n-    with S3Path(manifest_url).open(\"r\") as f:\n+    with (\n+        S3Path(manifest_url).open(\"r\")\n+        if manifest_url.startswith(\"s3://\")\n+        else open(manifest_url, \"r\")\n+    ) as f:\n         manifest = [json.loads(line) for line in f]\n     num_tokens = sum(int(line[\"num_sequences\"]) for line in manifest) * seqlen\n     return num_tokens\n@@ -46,13 +49,20 @@ def get_s3_dir_size(dataset_path):\n     return total_size\n \n \n+def get_dir_size(dataset_path):\n+    total_size = 0\n+    for filename in os.listdir(dataset_path):\n+        file_path = os.path.join(dataset_path, filename)\n+        if os.path.isfile(file_path):\n+            total_size += os.path.getsize(file_path)\n+    return total_size\n+\n def get_git_info():\n     repo = git.Repo(search_parent_directories=True)\n     dcnlp_commit_hash = repo.head.object.hexsha\n     dcnlp_diff = repo.git.diff(repo.head.commit.tree)\n     return dcnlp_commit_hash, dcnlp_diff\n \n-\n def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_key=\".json.zstd\"):\n     sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n     dcnlp_commit_hash, dcnlp_diff = get_git_info()",
    "data_key": "zstd"
}