{
    "uuid": "9e3c6d35-f73e-48d1-885f-25133cbf85f2",
    "name": "refinedweb",
    "creation_date": "2024_08_12-21_26_15",
    "dataset_url": "output/cc_wet_2019_april_baselines/refinedweb/refinedweb/processed_data/",
    "manifest_url": null,
    "sources": [
        {
            "uuid": "f12dc026-cc4a-4203-ba9f-9ba08c5945f9",
            "name": "CC_shard_00000000"
        }
    ],
    "tokenized": false,
    "tokenizer": null,
    "num_tokens": null,
    "size": 0,
    "dcnlp_commit_hash": "e954cd0b39ef0b593c1478ba666b47fa60469542",
    "dcnlp_diff": "diff --git a/baselines/baselines_configs/refinedweb.yaml b/baselines/baselines_configs/refinedweb.yaml\nindex 03310bc..6fb54a2 100644\n--- a/baselines/baselines_configs/refinedweb.yaml\n+++ b/baselines/baselines_configs/refinedweb.yaml\n@@ -3,11 +3,11 @@\n   steps:\n     - func: move_url_modifier   # Necessary because 'url' was not set up in these jsonls, was in page['metadata']['WARC-Target-URI']\n     - func: url_substring_filter\n-      banlist_from_fname: baselines/mappers/banlists/refinedweb_banned_domains_curated.txt\n+      banlist_from_fname: baselines/mappers/banlists/ldnoobw.txt\n       exact_domain_match: True\n       ignore_chars: ['www']\n     - func: url_substring_filter\n-      banlist: [\"arxiv.org\", \"askubuntu.com\", \"stackoverflow.com\", \"stackapps.com\", \n+      banlist: [\"arxiv.org\", \"askubuntu.com\", \"stackoverflow.com\", \"stackapps.com\",\n                 \"stackexchange.com\", \"mathoverflow.net\", \"exporter.nih.gov\", \"ncbi.nlm.nih.gov\",\n                 \"github.com\", \"irclogs.ubuntu.com\", \"news.ycombinator.com\", \"courtlistener.com\",\n                 \"reddit.com\", \"statmt.org\", \"uspto.gov\", \"en.wikipedia.org\"]\n@@ -24,7 +24,7 @@\n       num_banned_substrs: 2\n       match_substrings: False\n     - func: url_removal_modifier\n-    - func: newline_removal_modifier         \n+    - func: newline_removal_modifier\n       max_consecutive: 2\n     - func: detect_lang_whole_page_enricher\n       model: fasttext\n@@ -46,24 +46,24 @@\n     - func: bullet_count_filter\n       max_bullet_start_ratio: 0.9\n     - func: ellipsis_count_filter\n-      max_ellipsis_end_ratio: 0.3 \n-    - func: alphabetic_word_ratio_filter \n+      max_ellipsis_end_ratio: 0.3\n+    - func: alphabetic_word_ratio_filter\n       max_ratio: 0.2\n-    - func: stop_word_filter  \n-      count_unique: False                   \n+    - func: stop_word_filter\n+      count_unique: False\n       min_stop_word: 2\n     - func: massive_web_repetition_filters\n     - func: word_counter_enricher\n       key: previous_word_count\n     - func: uppercase_ratio_line_modifier\n-      max_ratio: 0.5                       \n-    - func: numeric_ratio_line_modifier    \n-      max_ratio: 0.999999                    \n-    - func: counter_line_modifier        \n+      max_ratio: 0.5\n+    - func: numeric_ratio_line_modifier\n+      max_ratio: 0.999999\n+    - func: counter_line_modifier\n     - func: line_length_modifier\n       min_length: 2\n-    - func: substring_line_modifier        \n-      max_length: 10 \n+    - func: substring_line_modifier\n+      max_length: 10\n       banlist: \"items in cart\"\n       remove_substring_only: True\n     - func: substring_line_modifier\n@@ -78,4 +78,4 @@\n       remove_substring_only: True\n     - func: word_removal_ratio_filter\n       prev_word_count_key: previous_word_count\n-      max_removed_ratio: 0.05\n\\ No newline at end of file\n+      max_removed_ratio: 0.05\ndiff --git a/ray_processing/process.py b/ray_processing/process.py\nindex 58cd867..0cede2b 100644\n--- a/ray_processing/process.py\n+++ b/ray_processing/process.py\n@@ -206,7 +206,8 @@ if __name__ == \"__main__\":\n                 working_dir = global_stats[i - 1][\"working_dir\"] if i > 0 else working_dir\n \n         # Retrieve the list of files before processing a chunk (in case of deletions)\n-        shard_files = list_shard_files(working_dir, args.num_shards, args.shard_list_file)\n+        # shard_files = list_shard_files(working_dir, args.num_shards, args.shard_list_file)\n+        shard_files = [\"CC_shard_00000000.jsonl.zst\"]\n         shard_extension = os.path.splitext(shard_files[0])[-1][1:]\n         print(f\"Starting chunk {i} with name {step_name}, # of input jsonls = {len(shard_files)}\")\n \ndiff --git a/ray_processing/utils.py b/ray_processing/utils.py\nindex f835848..7fbc897 100644\n--- a/ray_processing/utils.py\n+++ b/ray_processing/utils.py\n@@ -46,13 +46,20 @@ def get_s3_dir_size(dataset_path):\n     return total_size\n \n \n+def get_dir_size(dataset_path):\n+    total_size = 0\n+    for filename in os.listdir(dataset_path):\n+        file_path = os.path.join(dataset_path, filename)\n+        if os.path.isfile(file_path):\n+            total_size += os.path.getsize(file_path)\n+    return total_size\n+\n def get_git_info():\n     repo = git.Repo(search_parent_directories=True)\n     dcnlp_commit_hash = repo.head.object.hexsha\n     dcnlp_diff = repo.git.diff(repo.head.commit.tree)\n     return dcnlp_commit_hash, dcnlp_diff\n \n-\n def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_key=\".json.zstd\"):\n     sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n     dcnlp_commit_hash, dcnlp_diff = get_git_info()\n@@ -67,7 +74,7 @@ def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_\n         \"tokenized\": False,\n         \"tokenizer\": None,\n         \"num_tokens\": None,\n-        \"size\": get_s3_dir_size(args.output_dir),\n+        \"size\": get_dir_size(args.output_dir),\n         \"dcnlp_commit_hash\": dcnlp_commit_hash,\n         \"dcnlp_diff\": dcnlp_diff,\n         \"data_key\": data_key,\n@@ -99,7 +106,7 @@ def generate_tokenized_dataset_json(args, source_refs, data_key=\"json.gz\"):\n         \"tokenized\": True,\n         \"tokenizer\": args.tokenizer,\n         \"num_tokens\": count_tokens(manifest_url, args.seqlen + 1),\n-        \"size\": get_s3_dir_size(args.output),\n+        \"size\": get_dir_size(args.output),\n         \"dcnlp_commit_hash\": dcnlp_commit_hash,\n         \"dcnlp_diff\": dcnlp_diff,\n         \"data_key\": data_key,",
    "data_key": "zst"
}
