{
    "uuid": "95a9be54-f57d-4fd3-a5d7-bb693f4a94ba",
    "name": "refinedweb",
    "creation_date": "2024_08_22-01_15_06",
    "dataset_url": "output/cc_wet_2019_april_baselines/refinedweb_tokenized",
    "manifest_url": "output/cc_wet_2019_april_baselines/refinedweb_tokenized/manifest.jsonl",
    "sources": [
        {
            "uuid": "f12dc026-cc4a-4203-ba9f-9ba08c5945f9",
            "name": "CC_shard_00000000"
        }
    ],
    "tokenized": true,
    "tokenizer": "/data/users/zichunyu/out/hf/pythia-410m",
    "num_tokens": 0,
    "size": 275752960,
    "dcnlp_commit_hash": "6eb3a99f5d476e4de366cc5b3dca7c924094caea",
    "dcnlp_diff": "diff --git a/eval/eval_openlm_ckpt.py b/eval/eval_openlm_ckpt.py\nindex b4a5cba..91272dd 100644\n--- a/eval/eval_openlm_ckpt.py\n+++ b/eval/eval_openlm_ckpt.py\n@@ -523,7 +523,7 @@ def main():\n \n     CWD = os.getcwd()\n     if args.use_temp_working_dir:\n-        temp_dir = os.path.join(CWD, \"eval_openlm_ckpt_temp_dirs\", f\"{uuid.uuid4()}\")\n+        temp_dir = os.path.join(CWD, \"eval_openlm_ckpt_temp_dirs\")\n         os.makedirs(temp_dir, exist_ok=True)  # in case rank > 0\n         os.chdir(temp_dir)\n         print(f\"Using temporary working directory: {temp_dir}\")\ndiff --git a/ray_processing/process.py b/ray_processing/process.py\nindex 58cd867..0cede2b 100644\n--- a/ray_processing/process.py\n+++ b/ray_processing/process.py\n@@ -206,7 +206,8 @@ if __name__ == \"__main__\":\n                 working_dir = global_stats[i - 1][\"working_dir\"] if i > 0 else working_dir\n \n         # Retrieve the list of files before processing a chunk (in case of deletions)\n-        shard_files = list_shard_files(working_dir, args.num_shards, args.shard_list_file)\n+        # shard_files = list_shard_files(working_dir, args.num_shards, args.shard_list_file)\n+        shard_files = [\"CC_shard_00000000.jsonl.zst\"]\n         shard_extension = os.path.splitext(shard_files[0])[-1][1:]\n         print(f\"Starting chunk {i} with name {step_name}, # of input jsonls = {len(shard_files)}\")\n \ndiff --git a/ray_processing/tokenize_shuffle.py b/ray_processing/tokenize_shuffle.py\nindex 6a029d7..bbe9ba3 100644\n--- a/ray_processing/tokenize_shuffle.py\n+++ b/ray_processing/tokenize_shuffle.py\n@@ -102,9 +102,10 @@ def main(args, dcnlp_arg_names):\n     dataset_json = generate_tokenized_dataset_json(args, source_refs)\n     with open(json_path, \"w\") as ref_file:\n         json.dump(dataset_json, ref_file, indent=4)\n-    out_json_path = f\"{args.output}/{pathlib.Path(args.output).name}.json\"\n-    print(f\"moving dataset json to {out_json_path}\")\n-    os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n+    # out_json_path = f\"{args.output}/{pathlib.Path(args.output).name}.json\"\n+    # os.system(f\"aws s3 cp {json_path} {out_json_path}\")\n+    out_json_path = f\"{args.output.rstrip('/')}/{pathlib.Path(args.output).name}.json\"\n+    os.system(f\"cp {json_path} {out_json_path}\")\n \n \n if __name__ == \"__main__\":\ndiff --git a/ray_processing/utils.py b/ray_processing/utils.py\nindex f835848..347e1ba 100644\n--- a/ray_processing/utils.py\n+++ b/ray_processing/utils.py\n@@ -30,9 +30,12 @@ def get_source_ref(source_ref_path):\n     with open(source_ref_path, \"r\") as file:\n         return json.load(file)\n \n-\n def count_tokens(manifest_url, seqlen=2049):\n-    with S3Path(manifest_url).open(\"r\") as f:\n+    with (\n+        S3Path(manifest_url).open(\"r\")\n+        if manifest_url.startswith(\"s3://\")\n+        else open(manifest_url, \"r\")\n+    ) as f:\n         manifest = [json.loads(line) for line in f]\n     num_tokens = sum(int(line[\"num_sequences\"]) for line in manifest) * seqlen\n     return num_tokens\n@@ -46,13 +49,20 @@ def get_s3_dir_size(dataset_path):\n     return total_size\n \n \n+def get_dir_size(dataset_path):\n+    total_size = 0\n+    for filename in os.listdir(dataset_path):\n+        file_path = os.path.join(dataset_path, filename)\n+        if os.path.isfile(file_path):\n+            total_size += os.path.getsize(file_path)\n+    return total_size\n+\n def get_git_info():\n     repo = git.Repo(search_parent_directories=True)\n     dcnlp_commit_hash = repo.head.object.hexsha\n     dcnlp_diff = repo.git.diff(repo.head.commit.tree)\n     return dcnlp_commit_hash, dcnlp_diff\n \n-\n def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_key=\".json.zstd\"):\n     sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n     dcnlp_commit_hash, dcnlp_diff = get_git_info()\n@@ -67,7 +77,7 @@ def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_\n         \"tokenized\": False,\n         \"tokenizer\": None,\n         \"num_tokens\": None,\n-        \"size\": get_s3_dir_size(args.output_dir),\n+        \"size\": get_dir_size(args.output_dir),\n         \"dcnlp_commit_hash\": dcnlp_commit_hash,\n         \"dcnlp_diff\": dcnlp_diff,\n         \"data_key\": data_key,\n@@ -99,7 +109,7 @@ def generate_tokenized_dataset_json(args, source_refs, data_key=\"json.gz\"):\n         \"tokenized\": True,\n         \"tokenizer\": args.tokenizer,\n         \"num_tokens\": count_tokens(manifest_url, args.seqlen + 1),\n-        \"size\": get_s3_dir_size(args.output),\n+        \"size\": get_dir_size(args.output),\n         \"dcnlp_commit_hash\": dcnlp_commit_hash,\n         \"dcnlp_diff\": dcnlp_diff,\n         \"data_key\": data_key,",
    "data_key": "json.gz",
    "sampling_yaml": null
}